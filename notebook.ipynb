{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip3 install gradio\n",
    "%pip install opencv-python\n",
    "%pip install scikit-learn\n",
    "%pip install sentence-transformers\n",
    "%pip install transformers\n",
    "%pip install torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# handwriting reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import math\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parametricIntersect(r1:float, t1:float, r2:float, t2:float) -> (bool, int, int): # type: ignore\n",
    "    ct1:float=math.cos(t1)\n",
    "    st1:float=math.sin(t1)\n",
    "    ct2:float=math.cos(t2)\n",
    "    st2:float=math.sin(t2)\n",
    "    d=ct1*st2-st1*ct2 # determinative (rearranged matrix for inverse)\n",
    "    if d!= 0:\n",
    "        x=int((st2*r1-st1*r2)/d)\n",
    "        y=int((-ct2*r1+ct1*r2)/d)\n",
    "        return True, x, y\n",
    "    else: # //lines are parallel and will NEVER intersect!\n",
    "        return False, -1, -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chop_image(img_path, vertical_thresh):\n",
    "    img = cv2.imread(img_path)\n",
    "    # convert to black and white\n",
    "    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    edges = cv2.Canny(gray,50,150,apertureSize = 3)\n",
    "    lines = cv2.HoughLines(edges,1,np.pi/180,200)\n",
    "    vertical_threshold = int(img.shape[0] * vertical_thresh)\n",
    "    final_lines = []\n",
    "    skip_lines = []\n",
    "    if lines is None:\n",
    "        return [img]\n",
    "    for i in range(len(lines)):\n",
    "        rho1 = lines[i][0][0]\n",
    "        close_lines = []\n",
    "        for j in range(i+1, len(lines)):\n",
    "            rho2 = lines[j][0][0]\n",
    "            if i not in skip_lines and j not in skip_lines:\n",
    "                if abs(rho2 - rho1) < vertical_threshold:\n",
    "                    close_lines.append(lines[i][0])\n",
    "                    skip_lines.append(j)\n",
    "                if len(close_lines) == 0:\n",
    "                    final_lines.append(lines[i][0])\n",
    "                else:\n",
    "                    best_line = [math.inf, math.inf]\n",
    "                    for close_line in close_lines:\n",
    "                        if close_line[0] < best_line[0]:\n",
    "                            best_line = close_line\n",
    "                    final_lines.append(best_line)\n",
    "    final_final_lines = np.unique(np.array(final_lines), axis=0)\n",
    "    chopped_images = []\n",
    "    for line in range(len(final_final_lines)):\n",
    "        if line == 0:\n",
    "            y0 = 0\n",
    "        else:\n",
    "            y0 = int(final_final_lines[line - 1][0])\n",
    "        y1 = int(final_final_lines[line][0])\n",
    "        # print(y0, y1)\n",
    "        # img = cv2.imread(im_path)\n",
    "\n",
    "        # plt.imsave(f\"line{line}.png\", img[y0:y1, :, :])\n",
    "        chopped_images.append(img[y0:y1, :, :])\n",
    "    return chopped_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_path1 = \"/Users/isaac/Desktop/Creative/Coding/momApp/Beauty from Ashes Survey Data/handwriting/Screenshot 2024-04-21 at 5.09.43 PM.png\"\n",
    "im_path2 = \"/Users/isaac/Desktop/Creative/Coding/momApp/Beauty from Ashes Survey Data/handwriting/Screenshot 2024-04-21 at 5.09.43 PM copy.png\"\n",
    "chopped = chop_image(im_path2, 0.1)\n",
    "for i, img in enumerate(chopped):\n",
    "    plt.imsave(f\"line{i}.png\", img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-handwritten and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"image-to-text\", model=\"microsoft/trocr-base-handwritten\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I loved this conference. The speakers\n",
      " were great. Great activities and topics.\n",
      " Great location. A little far for me, but\n",
      " so worth it\n"
     ]
    }
   ],
   "source": [
    "for i, img in enumerate(chopped):\n",
    "    pil_im = Image.fromarray(img)\n",
    "    pil_im.save(f\"line{i}.png\")\n",
    "    text = pipe(pil_im)[0][\"generated_text\"]\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback = [\n",
    "        \"I loved this conference. The speakers were great, Great activities and topics. Great location. A little too far for me, but so worth it.\",\n",
    "        \"Very encouraging, love the first message and the last\",\n",
    "        \"I loved the overall themes and felt inspired to maintain mental health as well as stepping in to who God has called me to be\",\n",
    "        \"Gut health, We are loved by God, Change self talk\",\n",
    "        \"I love the fact that Jesus is the center of every speaker it is very refreshing\",\n",
    "        ]\n",
    "\n",
    "conference_theme = [\n",
    "    \"God is doing a new thing\",\n",
    "    \"A new thing\",\n",
    "    \"Season of Hope\",\n",
    "    \"Women Arise\",\n",
    "    \"What does being a leader look like under the leadership of your husband\",\n",
    "    \"Healthy Relationships\",\n",
    "    \"Medical health (any) as well as emotional health\",\n",
    "    \"More wooden bowls from zambia\",\n",
    "    \"A lesson on losses\"]\n",
    "\n",
    "house_names = [\n",
    "    \"House of Hope\",\n",
    "    \"Hope Arise\",\n",
    "    \"Arise Hope\",\n",
    "    \"She is my delight\",\n",
    "    \"My delight\",\n",
    "    \"Tirzah\",\n",
    "    \"Encompassed by Grace\",\n",
    "    \"Home of Grace\",\n",
    "    \"Grace Point\",\n",
    "    \"Hepzibah (my delight in her)\",\n",
    "    \"Angle aware\",\n",
    "    \"Child of God\",\n",
    "    \"Gods Love Children\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# ability to exclude certain words from process\n",
    "# better handwriting detection\n",
    "# fix problem with 1 or 0 line of text\n",
    "# fix issue with 0 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/isaac/Desktop/Creative/Coding/momApp/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-handwritten and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/private/var/folders/q7/brbj2bqs68lgmt6f_sklpqfh0000gn/T/gradio/bd6ec13cdb2a6065f04e689a929ee82b74aeb9e1/Screenshot 2024-04-21 at 5.09.43PM.png\n",
      "/private/var/folders/q7/brbj2bqs68lgmt6f_sklpqfh0000gn/T/gradio/102b8f6398445d022676540c7fbd14c2de0815f7/Screenshot 2024-04-21 at 5.09.43PM copy.png\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/isaac/Desktop/Creative/Coding/momApp/.venv/lib/python3.12/site-packages/transformers/generation/utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import cv2\n",
    "import math\n",
    "import gradio as gr\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def chop_image(img_path, vertical_thresh):\n",
    "    img = cv2.imread(img_path)\n",
    "    # convert to black and white\n",
    "    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    edges = cv2.Canny(gray,50,150,apertureSize = 3)\n",
    "    lines = cv2.HoughLines(edges,1,np.pi/180,200)\n",
    "    vertical_threshold = int(img.shape[0] * vertical_thresh)\n",
    "    final_lines = []\n",
    "    skip_lines = []\n",
    "    if lines is None:\n",
    "        return [img]\n",
    "    for i in range(len(lines)):\n",
    "        rho1 = lines[i][0][0]\n",
    "        close_lines = []\n",
    "        for j in range(i+1, len(lines)):\n",
    "            rho2 = lines[j][0][0]\n",
    "            if i not in skip_lines and j not in skip_lines:\n",
    "                if abs(rho2 - rho1) < vertical_threshold:\n",
    "                    close_lines.append(lines[i][0])\n",
    "                    skip_lines.append(j)\n",
    "                if len(close_lines) == 0:\n",
    "                    final_lines.append(lines[i][0])\n",
    "                else:\n",
    "                    best_line = [math.inf, math.inf]\n",
    "                    for close_line in close_lines:\n",
    "                        if close_line[0] < best_line[0]:\n",
    "                            best_line = close_line\n",
    "                    final_lines.append(best_line)\n",
    "    final_final_lines = np.unique(np.array(final_lines), axis=0)\n",
    "    chopped_images = []\n",
    "    for line in range(len(final_final_lines)):\n",
    "        if line == 0:\n",
    "            y0 = 0\n",
    "        else:\n",
    "            y0 = int(final_final_lines[line - 1][0])\n",
    "        y1 = int(final_final_lines[line][0])\n",
    "        # print(y0, y1)\n",
    "        # img = cv2.imread(im_path)\n",
    "\n",
    "        # plt.imsave(f\"line{line}.png\", img[y0:y1, :, :])\n",
    "        chopped_images.append(img[y0:y1, :, :])\n",
    "    return chopped_images\n",
    "\n",
    "def image_to_text(image_list):\n",
    "    single_line_text_images = []\n",
    "    output_text = \"\"\n",
    "    for image in image_list:\n",
    "        print(image)\n",
    "        single_line_text_images += chop_image(image, 0.1)\n",
    "    print(len(single_line_text_images))\n",
    "    for img in single_line_text_images:\n",
    "        pil_im = Image.fromarray(img)\n",
    "        text = pipe(pil_im)[0][\"generated_text\"]\n",
    "        output_text += f\"{text}\\n\\n\"\n",
    "        print()\n",
    "    return output_text\n",
    "\n",
    "def text_processing(texts, similarity_thresh, min_words, max_words):\n",
    "    vectorizer = TfidfVectorizer(stop_words=\"english\", ngram_range=(min_words, max_words)) # setup TFIDF\n",
    "    X = vectorizer.fit_transform([texts]) # apply TFIDF to text\n",
    "    words = vectorizer.get_feature_names_out() # words/phrases from TFIDF\n",
    "    scores = np.asarray(X.mean(axis=0))[0] # scores for above words/phrases from TFIDF\n",
    "    embeddings = model.encode(words) # embed words into latent space\n",
    "    normed = normalize(embeddings) # normalize embeddings\n",
    "    clustering = AgglomerativeClustering(n_clusters=None, distance_threshold=similarity_thresh) # set up clustering\n",
    "    clustering.fit(normed) # cluster words into groups\n",
    "    labels = clustering.labels_\n",
    "    # combine words and scores from clusters\n",
    "    df = None\n",
    "    for n in range(clustering.n_clusters_): # for each cluster\n",
    "        mask = (labels == n) # get all matching words in cluster\n",
    "        text, summ = \"\", 0\n",
    "        for word, score in zip(words[mask], scores[mask]): # for each word and score in cluster\n",
    "            text += f\"{word} \" # smoosh\n",
    "            summ += score # combine scores in cluster\n",
    "        text = \"/\".join(set(text.split(\" \")))\n",
    "        if df is None:\n",
    "            df = pd.DataFrame({\"Idea\":[text], \"Score\":[summ]}) # create dataframe\n",
    "        else:\n",
    "            df = pd.concat([df, pd.DataFrame({\"Idea\":[text], \"Score\":[summ]})]) # append to dataframe\n",
    "\n",
    "    df = df.round(3)\n",
    "    df.sort_values(by=\"Score\", ascending=False) # sort dataframe by score\n",
    "    return df\n",
    "\n",
    "def save_to_csv(df:pd.DataFrame):\n",
    "    print(os.getcwd()+\"output.csv\")\n",
    "    df.to_csv(os.getcwd()+\"/output.csv\")\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    # \"global\" variables\n",
    "    # embeddings = gr.State(None)\n",
    "    pipe = pipeline(\"image-to-text\", model=\"microsoft/trocr-base-handwritten\")\n",
    "    model = SentenceTransformer('sentence-transformers/paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "    with gr.Column() as outer_col:\n",
    "        with gr.Row() as row1:\n",
    "            grouping_strength = gr.Number(value=1.2, label=\"phrase grouping strength\", maximum=2, minimum=0, precision=2, step=0.01)\n",
    "            min_words = gr.Number(value=3, label=\"phrase length minimum\", precision=0, minimum=1)\n",
    "            max_words = gr.Number(value=5, label=\"phrase length maximum\", precision=0, minimum=1)\n",
    "        with gr.Row() as row2:\n",
    "            with gr.Column() as col1:\n",
    "                files = gr.Files(label=\"images\", file_types=[\"image\"])\n",
    "                process_button1 = gr.Button(value=\"Process\")\n",
    "                text_output = gr.Textbox(label=\"\")\n",
    "                process_button1.click(fn=image_to_text, inputs=[files], outputs=[text_output])\n",
    "            with gr.Column() as col2:\n",
    "                textbox = gr.Textbox(label=\"text to condense\")\n",
    "                process_button2 = gr.Button(value=\"Process\")\n",
    "                df = gr.DataFrame(wrap=True)\n",
    "                process_button2.click(fn=text_processing, inputs=[textbox, grouping_strength, min_words, max_words], outputs=[df])\n",
    "                save_to_csv_button = gr.Button(value=\"Save To CSV\")\n",
    "                save_to_csv_button.click(fn=save_to_csv, inputs=df)\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n"
     ]
    }
   ],
   "source": [
    "demo.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
